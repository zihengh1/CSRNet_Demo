{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "from model import CSRNet\n",
    "from utils import save_checkpoint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import cv2\n",
    "import dataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global args, best_prec1\n",
    "    \n",
    "    best_prec1 = 1e6\n",
    "    args = parser.parse_args(\"part_A_train.json part_A_val.json -p model_best.pth.tar 0 0\".split())\n",
    "    args.original_lr = 1e-6\n",
    "    args.lr = 1e-6\n",
    "    args.batch_size    = 1\n",
    "    args.momentum      = 0.95\n",
    "    args.decay         = 5*1e-4\n",
    "    args.start_epoch   = 0\n",
    "    args.epochs = 150\n",
    "    args.steps         = [-1,1,100,150]\n",
    "    args.scales        = [1,1,1,1]\n",
    "    args.workers = 4\n",
    "    args.seed = time.time()\n",
    "    args.print_freq = 30\n",
    "    \n",
    "    with open(args.train_json, 'r') as outfile:\n",
    "        train_list = json.load(outfile)\n",
    "    with open(args.test_json, 'r') as outfile:\n",
    "        val_list = json.load(outfile)\n",
    "    \n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "    \n",
    "    model = CSRNet()\n",
    "    model = model.cuda()\n",
    "    criterion = nn.MSELoss(size_average=False).cuda()\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
    "                                momentum=args.momentum,\n",
    "                                weight_decay=args.decay)\n",
    "    \n",
    "    if args.pre:\n",
    "        if os.path.isfile(args.pre):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.pre))\n",
    "            checkpoint = torch.load(args.pre)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_prec1 = checkpoint['best_prec1']\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(args.pre, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.pre))\n",
    "    else: \n",
    "        print(\"train a new model\")\n",
    "            \n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        train(train_list, model, criterion, optimizer, epoch)\n",
    "        prec1 = validate(val_list, model, criterion)\n",
    "        is_best = prec1 < best_prec1\n",
    "        best_prec1 = min(prec1, best_prec1)\n",
    "        print(' * best MAE {mae:.3f} '\n",
    "              .format(mae=best_prec1))\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': args.pre,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_prec1': best_prec1,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best,args.task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_list, model, criterion, optimizer, epoch):\n",
    "    losses = AverageMeter()\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset.listDataset(train_list,\n",
    "                       shuffle=True,\n",
    "                       transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "                   ]), \n",
    "                       train=True, \n",
    "                       seen=model.seen,\n",
    "                       batch_size=args.batch_size,\n",
    "                       num_workers=args.workers),\n",
    "        batch_size=args.batch_size)\n",
    "    print('epoch %d, processed %d samples, lr %.10f' % (epoch, epoch * len(train_loader.dataset), args.lr))\n",
    "    model.train()\n",
    "    end = time.time()\n",
    "\n",
    "    for i, (img, target) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        img = img.cuda()\n",
    "        img = Variable(img)\n",
    "        output = model(img)\n",
    "        \n",
    "        target = target.type(torch.FloatTensor).unsqueeze(0).cuda()\n",
    "        target = Variable(target)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        losses.update(loss.item(), img.size(0))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        \n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  .format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_list, model, criterion):\n",
    "    print ('begin test')\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "    dataset.listDataset(val_list,\n",
    "                   shuffle=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225]),\n",
    "                   ]),  train=False),\n",
    "    batch_size=args.batch_size)    \n",
    "    model.eval()\n",
    "    \n",
    "    mae = 0\n",
    "\n",
    "    for i,(img, target) in enumerate(test_loader):\n",
    "        img = img.cuda()\n",
    "        img = Variable(img)\n",
    "        output = model(img)\n",
    "        \n",
    "        mae += abs(output.data.sum()-target.sum().type(torch.FloatTensor).cuda())\n",
    "        \n",
    "    mae = mae/len(test_loader)    \n",
    "    print(' * MAE {mae:.3f} '\n",
    "              .format(mae=mae))\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    \n",
    "    args.lr = args.original_lr\n",
    "    \n",
    "    for i in range(len(args.steps)):\n",
    "        scale = args.scales[i] if i < len(args.scales) else 1\n",
    "        if epoch >= args.steps[i]:\n",
    "            args.lr = args.lr * scale\n",
    "            if epoch == args.steps[i]:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = args.lr\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=[], dest='task', nargs=None, const=None, default=None, type=<class 'str'>, choices=None, help='task id to use.', metavar='TASK')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch CSRNet')\n",
    "parser.add_argument('train_json', metavar='TRAIN',\n",
    "                    help='path to train json')\n",
    "parser.add_argument('test_json', metavar='TEST',\n",
    "                    help='path to test json')\n",
    "parser.add_argument('--pre', '-p', metavar='PRETRAINED', default=None, type=str,\n",
    "                    help='path to the pretrained model')\n",
    "parser.add_argument('gpu',metavar='GPU', type=str,\n",
    "                    help='GPU id to use.')\n",
    "parser.add_argument('task',metavar='TASK', type=str,\n",
    "                    help='task id to use.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train a new model\n",
      "epoch 0, processed 0 samples, lr 0.0000010000\n",
      "Epoch: [0][0/1200]\tTime 0.073 (0.073)\tData 0.018 (0.018)\tLoss 19.8724 (19.8724)\t\n",
      "Epoch: [0][30/1200]\tTime 0.870 (0.664)\tData 0.016 (0.017)\tLoss 155.6203 (461.6062)\t\n",
      "Epoch: [0][60/1200]\tTime 0.279 (0.663)\tData 0.015 (0.016)\tLoss 35.5283 (356.6390)\t\n",
      "Epoch: [0][90/1200]\tTime 0.881 (0.668)\tData 0.019 (0.016)\tLoss 135.5500 (343.4587)\t\n",
      "Epoch: [0][120/1200]\tTime 0.222 (0.667)\tData 0.004 (0.015)\tLoss 13.8012 (348.8259)\t\n",
      "Epoch: [0][150/1200]\tTime 0.975 (0.673)\tData 0.016 (0.015)\tLoss 433.3275 (411.7824)\t\n",
      "Epoch: [0][180/1200]\tTime 0.720 (0.674)\tData 0.023 (0.016)\tLoss 176.0151 (405.3468)\t\n",
      "Epoch: [0][210/1200]\tTime 0.882 (0.671)\tData 0.017 (0.015)\tLoss 89.6408 (397.1288)\t\n",
      "Epoch: [0][240/1200]\tTime 0.752 (0.675)\tData 0.021 (0.015)\tLoss 77.3975 (381.3692)\t\n",
      "Epoch: [0][270/1200]\tTime 0.541 (0.677)\tData 0.017 (0.015)\tLoss 130.3658 (390.7399)\t\n",
      "Epoch: [0][300/1200]\tTime 0.982 (0.679)\tData 0.012 (0.015)\tLoss 17.7729 (390.7248)\t\n",
      "Epoch: [0][330/1200]\tTime 0.723 (0.681)\tData 0.008 (0.015)\tLoss 95.5111 (431.2435)\t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c7bc734e5e35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-20580d3415a9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0madjust_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mprec1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mis_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprec1\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_prec1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-e73422eaa522>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_list, model, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mdata_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
